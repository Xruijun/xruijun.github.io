---
layout:     post   				    # 使用的布局（不需要改）
title:      读书笔记之《机器学习》周志华				# 标题 
subtitle:    #副标题
date:       2023-05-05 				# 时间
author:     By xrj						# 作者
header-img: img/post-bg-2015.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 机器学习
    - 读书笔记
---

## 第4章 决策树

### 4.1 基本流程

决策树学习希望得到泛化能力强的树，即能够处理没有见过的实例。

决策树递归返回有三种情况，（1）当前结点包含的样本同属于某个类别，无需划分；（2）当前结点的属性集为空，或所有样本的属性完全相同，无法再进行划分，但要设定一个新的类别，对应的就是样本的属性；（3）当前结点包含的样本集合为空，不能划分，但其类别设定为父节点包含最多的样本的类别。

### 4.2 划分选择

#### 4.2.1 信息增益

如果分支结点包含的样本尽可能属于同一类，那么结点的纯度越来越高。信息熵是度量样本纯度的一种指标，信息熵越低，认为纯度越高。

回到递归算法，如何选出最优的属性来进行划分？引入了新的概念，信息增益。对于属性a样本集D而言，信息增益越大，认为属性a对样本集D的纯度提升越高，因此我们把所有属性对样本集D的信息增益算出来，取最大值即为最优的划分属性。

#### 4.2.2 增益率

信息增益有个缺点，比如西瓜的编号为1-20，把这个属性加上去，得到的纯度必然是最大的，因为每个样本都代表着一个类别。也就是说，信息增益对有多种取值的属性有偏好，这种偏好会带来不利影响。

增益率减小了对多种取值的属性的偏好，与信息增益类似，增益率越大，认为对样本集D的纯度提升越高。但是，它自己又偏好于取值较少的属性。那么，C4.5决策树算法采用启发式：先根据信息增益选出高于平均水平的属性，然后在这些属性中，根据最高的增益率选出最优属性。

#### 4.2.3 基尼指数

基尼指数越小，认为样本集D的纯度越高。与上面类似，选出使得基尼指数最低的属性。

### 4.3 剪枝处理

剪枝处理是对付决策树的过拟合的方法。将训练集划分一部分出来作为验证集，用来计算精准度。

#### 4.3.1 预剪枝

在对某个结点划分之前，计算当前的精准度，如果划分后，精准度上升就允许划分；如果精准度下降就不允许划分。但是，可能出现在一种情况，这一层划分后精准度下降，下一层划分后精准度上升，带来了欠拟合的风险。

#### 4.3.2 后剪枝

生成一颗完整的决策树后，倒退回去验证精准度。将子树的根节点替换为叶子结点，加入精准度提高就替代，否则就不替代。与预剪枝相比，后剪枝减小了欠拟合的风险，但是由于生成一颗完整的决策树，会花费更多时间。

### 4.4 连续值与缺省值

### 4.5 多变量决策树

## 第7章 贝叶斯分类器

### 7.1 贝叶斯决策论

通过贝叶斯公式计算预测结果

$$P(Ci|X) = \frac{P(Ci)P(X|Ci)}{P(X)}$$

根据大数理论，P(Ci)可根据样本中出现的频率替代概率；

$$P(X) = \sum_{i=0}^{Ni}P(X|Ci)P(Ci)$$

P(X)与样本所属类别无关；

因此实际概率值只与先验概率和似然（类条件概率）有关。

### 7.2 极大似然估计

估计似然（类条件概率）P(X|Ci)的常用策略是**假定**其具有某种特殊的分布，比如高斯分布（正态分布），伯努利分布，多项式分布等等。这些分布具有唯一的参数确定，训练过程就是找出使得概率最大的参数。

要注意，上文说到是假定一个模型，真实的数据分布不一定符合这个模型，需要根据经验去选择合适的模型。或者也可以几个模型都用用，取预测效果最好的那个模型。

### 7.3 朴素贝叶斯分类器

似然的问题在于后验概率是所有属性的联合分布，难以从样本中得到不同属性之间的关系。朴素贝叶斯提出一个“属性条件独立性假设”，就是假设这些属性之间互相独立。

$$
P(X|Ci) = \prod_{i=0}^{d}{P(Xi|Ci)}
$$

那么似然就等于每个属性的类条件概率的累乘。在实际代码中，累乘会带来下溢的影响，即太小会等于0，处理方式是取对数做加法。

同时，还有平滑操作。样本中可能有些情况没有出现，导致概率为0，这会导致整个概率都等于0，即使取对数也会导致无穷小。常用的平滑操作是拉普拉斯修正。

在写码过程中，还有不同的技巧。对于给定的训练集，可以先计算出所有的概率估值，存储起来，当需要预测的时候查表即可；对于训练样本频繁更改的情况，可以采用懒惰学习，只在需要预测的时候才计算概率估值；对于不断增加数据的情况，可在现有估值的基础上，只修改会发生变化的项。

### 7.4 半朴素贝叶斯

实际生活中，属性独立分布很难满足。

### 7.5 贝叶斯网

### 7.6 EM算法
