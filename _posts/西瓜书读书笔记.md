---
layout:     post   				    # 使用的布局（不需要改）
title:      读书笔记之《机器学习》周志华				# 标题 
subtitle:   Hello World, Hello Blog #副标题
date:       2023-05-05 				# 时间
author:     By xrj						# 作者
header-img: img/post-bg-2015.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 生活
---

## 第4章 决策树

### 4.1 基本流程

决策树学习希望得到泛化能力强的树，即能够处理没有见过的实例。

决策树递归返回有三种情况，（1）当前结点包含的样本同属于某个类别，无需划分；（2）当前结点的属性集为空，或所有样本的属性完全相同，无法再进行划分，但要设定一个新的类别，对应的就是样本的属性；（3）当前结点包含的样本集合为空，不能划分，但其类别设定为父节点包含最多的样本的类别。

### 4.2 划分选择

#### 4.2.1 信息增益

如果分支结点包含的样本尽可能属于同一类，那么结点的纯度越来越高。信息熵是度量样本纯度的一种指标，信息熵越低，认为纯度越高。

回到递归算法，如何选出最优的属性来进行划分？引入了新的概念，信息增益。对于属性a样本集D而言，信息增益越大，认为属性a对样本集D的纯度提升越高，因此我们把所有属性对样本集D的信息增益算出来，取最大值即为最优的划分属性。

#### 4.2.2 增益率

信息增益有个缺点，比如西瓜的编号为1-20，把这个属性加上去，得到的纯度必然是最大的，因为每个样本都代表着一个类别。也就是说，信息增益对有多种取值的属性有偏好，这种偏好会带来不利影响。

增益率减小了对多种取值的属性的偏好，与信息增益类似，增益率越大，认为对样本集D的纯度提升越高。但是，它自己又偏好于取值较少的属性。那么，C4.5决策树算法采用启发式：先根据信息增益选出高于平均水平的属性，然后在这些属性中，根据最高的增益率选出最优属性。

#### 4.2.3 基尼指数

基尼指数越小，认为样本集D的纯度越高。与上面类似，选出使得基尼指数最低的属性。

### 4.3 剪枝处理

剪枝处理是对付决策树的过拟合的方法。将训练集划分一部分出来作为验证集，用来计算精准度。

#### 4.3.1 预剪枝

在对某个结点划分之前，计算当前的精准度，如果划分后，精准度上升就允许划分；如果精准度下降就不允许划分。但是，可能出现在一种情况，这一层划分后精准度下降，下一层划分后精准度上升，带来了欠拟合的风险。

#### 4.3.2 后剪枝

生成一颗完整的决策树后，倒退回去验证精准度。将子树的根节点替换为叶子结点，加入精准度提高就替代，否则就不替代。与预剪枝相比，后剪枝减小了欠拟合的风险，但是由于生成一颗完整的决策树，会花费更多时间。
